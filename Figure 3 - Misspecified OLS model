import numpy as np
from numpy.random import default_rng
from scipy.stats import norm
from scipy.stats import t
from scipy.stats import uniform

rng = default_rng()

import matplotlib.pyplot as plt

########### Setting:
p=25
n=50
N=5*10**4
b0=1.5
b02_Mat=np.zeros((p,p))+b0**2

mu=np.zeros(p)

rho=0.9
Sigma=np.zeros((p,p))
r = 5 #number of blocks
block = p/r #size of a block

for j in range(r):
    i0 = j*block
    i1 = (j+1)*block-1
    i0=int(i0)
    i1=int(i1)
    Sigma[i0:i1+1,i0:i1+1] = rho


Sigma=(Sigma+np.eye(p)*(1-rho))


Xst0 = rng.multivariate_normal(mu,Sigma,N)
Xst1 = (norm.cdf(Xst0)-0.5)*np.sqrt(12)     #transforming to Unif Dist

Sigma0= Xst0.transpose()@Xst0/N
Sigma0_inv=np.linalg.inv(Sigma0)

Sigma1= Xst1.transpose()@Xst1/N
Sigma1_inv=np.linalg.inv(Sigma1)




Sigmas=[Sigma0,Sigma1]
Sigmas_inv=[Sigma0_inv,Sigma1_inv]
Xsts=[Xst0,Xst1]
D_lev=2

Bias_Vals=[0.3,0.6]
Bias_lev=2

Samp_size=5*10**3


ones_n=np.ones((n,1))
X_m=np.zeros((p,1))

##################### Unsupervised estimation: #############################
Bs=np.zeros(D_lev)
Vs=np.zeros(D_lev)


Ip=np.eye(p)
Qmats=[]
HDmats=[]
for d in range(D_lev):
  V=0
  B=0
  SigmaU=Sigmas[d]
  SigmaU_inv=Sigmas_inv[d]
  Xst=Xsts[d]
  HD=np.zeros((p,p))
  Q=np.zeros((p,p))

  for i in range(Samp_size):
    tr_id=rng.choice(range(N),size=n, replace=False)
    X = Xst[tr_id,:]
    HQ_mat=n*SigmaU@np.linalg.inv((X.transpose())@X )
    V=V+HQ_mat.trace()
    HH2_mat=(SigmaU_inv/n)@(X.transpose())@X@(X.transpose())@X-n*SigmaU
    B=B+HH2_mat.trace()

    mat0=(X.T)@X
    Hb=mat0/n
    mat1=np.linalg.inv(mat0 )
    HD+=mat0@(SigmaU_inv/n)@mat0
    Q+=mat1


  Vs[d]=V/Samp_size
  Bs[d]=B/Samp_size
  HDmats.append(HD/Samp_size)
  Qmats.append(Q/Samp_size)


sig_vals=np.sqrt(np.arange(10,320,15))
sig_lev=sig_vals.shape[0]


################# Full simulation:
Results=np.empty((D_lev,5,sig_lev,Samp_size,Bias_lev))

e=np.zeros( (n,1))

Ip=np.eye(p)

for Blev in range(Bias_lev):
  delta=Bias_Vals[Blev]

  for d in range(D_lev):
    SigmaU=Sigmas[d]
    SigmaU_inv=Sigmas_inv[d]
    Xst=Xsts[d]
    Vl=Vs[d]
    Bu=(1-1/n)*(1-1/(n*(p+1)))*Bs[d]
    HD=HDmats[d]
    Q=Qmats[d]

    mtes=np.reshape(b0*np.sum(Xst,1)+delta*np.sum(np.abs(Xst),1),(N,1))

    for i in range(Samp_size):
      tr_id=rng.choice(range(N),size=n, replace=False)
      beta=rng.multivariate_normal(mu,np.eye(p),1).transpose()

      #mtes=Xst@beta
      X = Xst[tr_id,:]
      ymeans=np.reshape(b0*np.sum(X,1)+delta*np.sum(np.abs(X),1),(n,1) )

      for sig in range(sig_lev):
        sige=sig_vals[sig]
        e[:,0]=rng.standard_normal(n)
        y=ymeans+e*sige

        yh_null= 0 #np.mean(y)

    ############# supervised OLS:
        beta_OLS=np.linalg.inv((X.transpose())@X )@(X.transpose())@y

    ############# Semi-supervised_Tilde:
        beta_SOLS1=SigmaU_inv@(X.transpose())@y/n
    ############# Semi-supervised_Breve:
        X_m[:,0]=np.mean(X,0)
        mean_mat=(ones_n)@(X_m.transpose())
        Xc=X-mean_mat
        cov_Xy=( (X-mean_mat).transpose())@( y-np.mean(y) )/n

        beta_SOLS2=SigmaU_inv@cov_Xy
    ################################ Estimating Noise:
        RSS=np.sum( (X@beta_OLS-y)**2 )
        sig_h=RSS/(n-p)

    ################################ Estimating Signal:
        tau_h=np.maximum( (np.mean(y**2)-sig_h)/SigmaU.trace() ,0.01)


    ################################ Estimating  the bias term:
        beta_SOLS2Vec=beta_SOLS2.reshape(p,1)
        B_h=np.maximum(  ( ( (HD-n*SigmaU)@beta_SOLS2Vec@beta_SOLS2Vec.T).trace() -((n-1)/n)*sig_h*((HD-n*SigmaU)@Q).trace() )/n, 0.01)


        ############## Estimating Rs:
        #test_Rs0= tau_h*SigmaU.trace()
        test_Rs1= sig_h*Vl/n
        test_Rs2= sig_h*p/n+B_h
        test_Rs3= test_Rs2*(n-1)/n
        Imin=np.argmin([ test_Rs1,test_Rs3 ])


        ##############################
        Rhat=np.mean( (Xst@beta_OLS -  mtes)**2 )
        Results[d,0,sig,i,Blev]=Rhat

        Rtilde=np.mean( (Xst@beta_SOLS1 -  mtes)**2 )
        Results[d,1,sig,i,Blev]=Rtilde

        Rbreve=np.mean( (Xst@beta_SOLS2 -  mtes)**2 )
        Results[d,2,sig,i,Blev]=Rbreve

        
        
        R0=np.mean( (yh_null -  mtes)**2 )
        Results[d,3,sig,i,Blev]=R0

        Results[d,4,sig,i,Blev]= [ Rhat,Rbreve ][Imin]


      if i%1000==999:
        print(i+1,'samples so far')


Results=np.mean(Results,axis=3)



### Plotting:

x2=sig_vals**2
fig = plt.figure()
gs = fig.add_gridspec(nrows=1, ncols=4, wspace=0.15)
axs = gs.subplots(sharex=False, sharey=False)

fig.set_size_inches(20,6)
s1=1.5
s2=5
s4=11

d=0
b=0
a=0
SigmaU=Sigmas[d]
axs[a].plot(x2[6::],Results[d,0,6::,b],'ro--', linewidth=s1, markersize=s2)
axs[a].plot(x2[6::],Results[d,1,6::,b],'o--', linewidth=s1, markersize=s2, color='green')
axs[a].plot(x2[6::],Results[d,2,6::,b],'o--', linewidth=s1, markersize=s2, color='blue')
axs[a].plot(x2[6::],Results[d,4,6::,b],'yo--', linewidth=s1, markersize=s2)
axs[a].hlines(Results[d,3,0,b],x2[6],x2[-1],linewidth=s1, color='black',linestyle='--')
axs[a].set_title('Gaussian, Low Bias', fontsize=15)
axs[a].legend( [r'$R(\hat{\beta})$', r'$R(\tilde{\beta})$', r'$R(\breve{\beta})$', r'$R(\beta^D)$', r'$R(0)$'],   fontsize=s4)
axs[a].set_xlabel(r'$\sigma^2$', fontsize=s4)




d=1
b=0
a=1
SigmaU=Sigmas[d]
axs[a].plot(x2[6::],Results[d,0,6::,b],'ro--', linewidth=s1, markersize=s2)
axs[a].plot(x2[6::],Results[d,1,6::,b],'o--', linewidth=s1, markersize=s2, color='green')
axs[a].plot(x2[6::],Results[d,2,6::,b],'o--', linewidth=s1, markersize=s2, color='blue')
axs[a].plot(x2[6::],Results[d,4,6::,b],'yo--', linewidth=s1, markersize=s2)
axs[a].hlines(Results[d,3,0,b],x2[6],x2[-1],linewidth=s1, color='black',linestyle='--')
axs[a].set_xlabel(r'$\sigma^2$', fontsize=s4)
axs[a].set_title('Uniform, Low Bias ', fontsize=15)



d=0
b=1
a=2
SigmaU=Sigmas[d]
axs[a].plot(x2,Results[d,0,:,b],'ro--', linewidth=s1, markersize=s2)
axs[a].plot(x2,Results[d,1,:,b],'o--', linewidth=s1, markersize=s2, color='green')
axs[a].plot(x2,Results[d,2,:,b],'o--', linewidth=s1, markersize=s2, color='blue')
axs[a].plot(x2,Results[d,4,:,b],'yo--', linewidth=s1, markersize=s2)
axs[a].hlines(Results[d,3,0,b],x2[0],x2[-1],linewidth=s1, color='black',linestyle='--')
axs[a].set_title('Gaussian, High Bias', fontsize=15)
axs[a].legend( [r'$R(\hat{\beta})$', r'$R(\tilde{\beta})$', r'$R(\breve{\beta})$', r'$R(\beta^D)$', r'$R(0)$'],   fontsize=s4)
axs[a].set_xlabel(r'$\sigma^2$', fontsize=s4)


d=1
b=1
a=3
SigmaU=Sigmas[d]
axs[a].plot(x2,Results[d,0,:,b],'ro--', linewidth=s1, markersize=s2)
axs[a].plot(x2,Results[d,1,:,b],'o--', linewidth=s1, markersize=s2, color='green')
axs[a].plot(x2,Results[d,2,:,b],'o--', linewidth=s1, markersize=s2, color='blue')
axs[a].plot(x2,Results[d,4,:,b],'yo--', linewidth=s1, markersize=s2)
axs[a].hlines(Results[d,3,0,b],x2[0],x2[-1],linewidth=s1, color='black',linestyle='--')
axs[a].set_title('Uniform, High Bias', fontsize=15)
axs[a].legend( [r'$R(\hat{\beta})$', r'$R(\tilde{\beta})$', r'$R(\breve{\beta})$', r'$R(\beta^D)$', r'$R(0)$'],   fontsize=s4)
axs[a].set_xlabel(r'$\sigma^2$', fontsize=s4)







